{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKBPM0DEjOPp"
   },
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is Machine Learning\n",
    "The science of getting machines to interpret, process and analyze data in order to solve real-world problems.\n",
    "\n",
    "### Branches of ML\n",
    "<img src=\"https://drive.google.com/uc?id=19SpxzT8uFCQt2nd9Xn7KNbnlyP-_DYbU\" width=\"650\">\n",
    "\n",
    "### Supervised Learning\n",
    "* In supervised learning, the ML model is trained based on the given input and its expected output, i.e., the label of the input. The model creates a mapping equation based on the inputs and outputs and predicts the label of the inputs in the future based on that mapping equation.\n",
    "\n",
    "* Let’s suppose we have to develop a model that differentiates between a cat and a dog. To train the model, we feed multiple images of cats and dogs into the model with a label indicating whether the image is of a cat or a dog. The model tries to develop an equation between the input images and their labels. After training, the model can predict whether an image is of a cat or a dog even if the image is previously unseen by the model.\n",
    "\n",
    "Below is an image of depicting the process of a Supervised model for a classification problem\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1qnNZhhuzoMiZVEWPCQMj_KHBFBsh2cL6\" width=\"750\">\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "* In unsupervised learning, the ML model is trained only on the inputs, without their labels. The model classifies the input data into classes that have similar features. The label of the input is then predicted in the future based on the similarity of its features with one of the classes.\n",
    "\n",
    "* Suppose we have a collection of red and blue balls and we have to classify them into two classes. Let’s say all other features of the balls are the same except for their color. The model tries to find the dissimilar features between the balls on the basis of how the model can classify the balls into two classes. After the balls are classified into two classes depending on their color, we get two clusters of balls, one of blue color and one of red color.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Ji62gIk8UoaQIX6HvoiJCrst_e58iFbx\" width=\"750\">\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "* In reinforcement learning, the AI model tries to take the best possible action in a given situation to maximize the total profit. The model learns by getting feedback on its past outcomes.\n",
    "\n",
    "* Consider the example of a robot that is asked to choose a path between A and B. In the beginning, the robot chooses either of the paths as it has no past experience. The robot is given feedback on the path it chooses and learns from this feedback. The next time the robot gets into a similar situation, it can use feedback to solve the problem. For example, if the robot chooses path B and gets a reward, i.e., positive feedback, this time the robot knows that it has to choose path B to maximize its reward.\n",
    "\n",
    "### Objective\n",
    "Our focus today will be on a Supervised ML Problem, Classification problem, to be precise. We will be using Machine Learning to evaluate the condition of a car based on it's characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gRc4OoTh139"
   },
   "source": [
    "The data we'll be working with was sourced from https://archive.ics.uci.edu/dataset/19/car+evaluation\n",
    "\n",
    "Each car evaluation is described with 7 attributes. 6 of the attributes represent car characteristics, such as buying price, price of the maintenance, number of doors, capacity in terms of persons to carry, the size of luggage boot, and estimated safety of the car. The seventh variable(our target variable) represents the evaluation of the car (unacceptable, acceptable, good, very good). Our goal is to predict the evaluation of the cars based on the available characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGzNpqfoyE_M"
   },
   "source": [
    "#### Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaHJCg3WeYwS"
   },
   "outputs": [],
   "source": [
    "# Uncomment only if you need it\n",
    "# !pip install pandas_profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJMj3_Bph14C"
   },
   "outputs": [],
   "source": [
    "# loading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# save model\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dzhiY72h14D"
   },
   "outputs": [],
   "source": [
    "# loading the .data file into our environment\n",
    "df = pd.read_csv(\"car.data\")\n",
    "df.columns = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN8V673n0gi0"
   },
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V_MR97Lh14F"
   },
   "outputs": [],
   "source": [
    "ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRZ_PT0k7z09"
   },
   "source": [
    "The data we are using today does not need a lot of cleaning and/or manipulation for modeling, which is very critical for any ML project. Here is what you should be aware of;\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Data cleaning is usually the most time-consuming stage of the Data Science process. This stage may take up to 50-80% of a Data Scientist's time as there are a vast number of possible problems that make the data \"dirty\" and unsuitable for analysis. Some of the problems you may see in data are:\n",
    "\n",
    "* Inconsistencies in data\n",
    "* Misspelled text data\n",
    "* Outliers\n",
    "* Imbalanced data\n",
    "* Invalid/outdated data\n",
    "* Missing data\n",
    "\n",
    "This stage requires the development of a careful strategy on how to deal with these issues. Such a strategy may vary substantially between different analyses depending on the nature of problems being solved.\n",
    "\n",
    "\n",
    "<!-- <img src='images/image_data_cleaning_corrected.png' width=\"600\"> -->\n",
    "<img src='https://drive.google.com/uc?id=1AlBs7SUNdjLjuStKHgC0yAjLI_saDfRX' width=\"600\">\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "Data exploration or Exploratory Data Analysis (EDA) helps highlight the patterns and relations in data. Exploratory analysis may involve the following activities:\n",
    "\n",
    "* Calculating basic descriptive statistics such as the mean, the median, and the mode\n",
    "* Creating a range of plots including histograms, scatter plots, and distribution curves to identify trends in the data\n",
    "* Other interactive visualizations to focus on a specific segments of data\n",
    "\n",
    "<!-- <img src='images/image_exploration.png' width=\"500\"> -->\n",
    "<img src='https://drive.google.com/uc?id=1t557oPYny8kaXIs90jmt0wtUbxsnrxfT' width=\"500\">\n",
    "\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "A \"Feature\" is a measurable attribute of the phenomenon being observed - the number of bedrooms in a house or the weight of a vehicle. Based on the nature of the analytical question asked in the first step, a Data Scientist may have to engineer additional features not found in the original dataset. Feature engineering is the process of using expert knowledge to transform raw data into meaningful features that directly address the problem you are trying to solve. For example, taking weight and height to calculate Body Mass Index for the individuals in the dataset. This stage will substantially influence the accuracy of the predictive model you construct in the next stage.\n",
    "\n",
    "<!-- <img src='images/image_engineering.png' width=\"500\"> -->\n",
    "<img src='https://drive.google.com/uc?id=184P4HEH6hs-YbQCFDdxPkvkj6AytYbYN' width=\"500\">\n",
    "\n",
    "### Data Visualization\n",
    "\n",
    "After deriving the required results from a statistical model, visualizations are normally used to summarize and present the findings of the analysis process in a form which is easily understandable by non-technical decision makers.\n",
    "\n",
    "Data visualization could be thought of as an evolution of visual communication techniques as it deals with the visual representation of data. There are a wide range of different data visualization techniques, from bar graphs, line graphs and scatter plots to alluvial diagrams and spatio-temporal visualizations, each of which will work better for presenting certain types of information.\n",
    "\n",
    "<!-- <img src='images/image_visualization.png' width=\"650\"> -->\n",
    "<img src='https://drive.google.com/uc?id=1ocxISohIsl1vgcEUKEbCd_HRYxMI2rih' width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBVc6mDVh14G"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLg0T_Rf2MKa"
   },
   "source": [
    "Machine learning algorithms cannot work with categorical data, the first thing to do ,therefore, is to convert our categorical variables into integers.\n",
    "Due to the type of data we have, we will perform ordinal encoding, which will convert the data while still maintaining the weights of the ratings, i.e better rating more weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkY6iE71h14H"
   },
   "outputs": [],
   "source": [
    "# First, we create an object 'column'_mapper in which we pass the encoding parameter. We do this for all the columns\n",
    "\n",
    "# creating object\n",
    "buying_mapper = {\n",
    "    \"low\": 1,\n",
    "    \"med\": 2,\n",
    "    \"high\": 3,\n",
    "    \"vhigh\": 4,\n",
    "}  # we'll use this on the maint column as well\n",
    "\n",
    "lugboot_mapper = {\"small\": 1, \"med\": 2, \"big\": 3}\n",
    "\n",
    "safety_mapper = {\"low\": 1, \"med\": 2, \"high\": 3}\n",
    "\n",
    "class_mapper = {\"unacc\": 1, \"acc\": 2, \"good\": 3, \"vgood\": 4}\n",
    "\n",
    "# passing parameter\n",
    "df[\"buying_enc\"] = df[\"buying\"].replace(buying_mapper)\n",
    "df[\"maint_enc\"] = df[\"maint\"].replace(buying_mapper)\n",
    "df[\"lugboot_enc\"] = df[\"lug_boot\"].replace(lugboot_mapper)\n",
    "df[\"safety_enc\"] = df[\"safety\"].replace(safety_mapper)\n",
    "df[\"class_enc\"] = df[\"class\"].replace(class_mapper)\n",
    "\n",
    "# assigning 5more in 'doors' and more in 'persons' with 5\n",
    "df.loc[df[\"doors\"] == \"5more\", \"doors\"] = 5\n",
    "df.loc[df[\"persons\"] == \"more\", \"persons\"] = 5\n",
    "\n",
    "# visualize the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv5kdEVwh14I"
   },
   "outputs": [],
   "source": [
    "# Extract the columns we will be using to build our model and save it in a diferent dataframe\n",
    "\n",
    "ml_df = df[\n",
    "    [\n",
    "        \"buying_enc\",\n",
    "        \"maint_enc\",\n",
    "        \"lugboot_enc\",\n",
    "        \"safety_enc\",\n",
    "        \"doors\",\n",
    "        \"persons\",\n",
    "        \"class_enc\",\n",
    "    ]\n",
    "]\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZP1C8qYh14J"
   },
   "outputs": [],
   "source": [
    "# checking the distribution of the target class using a countplot\n",
    "sns.countplot(x=ml_df[\"class_enc\"], data=ml_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYdXFzkHh14K"
   },
   "outputs": [],
   "source": [
    "# in order for our model to be generalize well on future data we have to deal with class imbalance by\n",
    "# oversampling the minority class. Before that, let's check how the distribution is across the other variables\n",
    "\n",
    "\n",
    "for i in ml_df.columns[\n",
    "    :-1\n",
    "]:  # selecting all columns except the last column, i.e. ml_df.drop(columns=['rating_mapped'])\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(\"Countplot for {} variable\".format(i))\n",
    "    sns.countplot(x=i, data=ml_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Iapikt7h14L"
   },
   "outputs": [],
   "source": [
    "ml_df[\"doors\"] = ml_df[\"doors\"].astype(int)\n",
    "ml_df[\"persons\"] = ml_df[\"persons\"].astype(int)\n",
    "ml_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYWSjWV_h14L"
   },
   "outputs": [],
   "source": [
    "# checking the pearson correlation of the df\n",
    "corr = ml_df.corr()\n",
    "matrix = np.triu(np.ones_like(corr))\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(corr, annot=True, mask=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1xIW19Ah14M"
   },
   "source": [
    "The correlation between the variables is very low, thus no reason to plot individual relationships between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhYCbUQHh14M"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSBexUks-VUU"
   },
   "source": [
    "Modeling is the stage where you use mathematical and/or statistical approaches to answer your analytical question. Predictive Modeling refers to the process of using probabilistic statistical methods to try to predict the outcome of an event. For example, based on employee data, an organization can develop a predictive model to identify employee attrition rate in order to develop better retention strategies.\n",
    "\n",
    "Choosing the \"right\" model is often a challenging decision as there is never a single right answer. Selecting a model involves balancing the accuracy and computational cost of the analysis process. For example, some recent approaches in predictive modeling such as deep learning have been shown to offer vastly improved accuracy of results, but with a very high computational cost.\n",
    "\n",
    "<!-- <img src='images/image_predictive.png' width=\"250\"> -->\n",
    "<img src='https://drive.google.com/uc?id=1WhUFiZClZYV0pxZ-fTs7NFPOSy9Dm50X' width=\"250\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgBUR3jAh14M"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into X(independent variables) and y(target variables)\n",
    "X = ml_df[ml_df.columns[:-1]]\n",
    "y = ml_df[\"class_enc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMP_QcTFh14M"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into X(independent variables) and y(target variables)\n",
    "X = ml_df[ml_df.columns[:-1]]\n",
    "y = ml_df[\"class_enc\"]\n",
    "\n",
    "# Then divide into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=23\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuLUACwbh14M"
   },
   "source": [
    "### Multinomial Logistic Regression Model\n",
    "\n",
    " Our dataset involves classifying cars into multiple categories, such as \"unacceptable,\" \"acceptable,\" \"good,\" and \"very good.\" Since there are more than two distinct classes, a multinomial logistic regression model is suitable for handling multiclass classification problems. It s specifically designed to model and predict outcomes with multiple categories.\n",
    "\n",
    "It is a relatively simple and computationally efficient algorithm and makes it a practical choice for smaller datasets,especially when interpretability is valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esZrNxPkh14M"
   },
   "source": [
    "Before we fit our model, we first need to normalize the data.\n",
    "\n",
    "The main idea behind normalization/standardization is that variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. Thus, to deal with this potential problem feature-wise normalization such as MinMax Scaling is usually used prior to model fitting.\n",
    "\n",
    "When we perform scaling on our models, it is often easier to take the entire dataset, scale it, then cross validate. However, what this does is allow data leakage. Data leakage in machine learning is a big problem. When we build machine learning models, we do everything we can to avoid training our model with anything from the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xhd3bcb3h14N"
   },
   "outputs": [],
   "source": [
    "# Let's create a pipeline that applies scaling on each fold\n",
    "\n",
    "# declare the steps in our pipeline\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"minmaxcaler\", MinMaxScaler()),\n",
    "        (\n",
    "            \"logisticregression\",\n",
    "            LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJlKcm_yh14N"
   },
   "outputs": [],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1689238883358,
     "user": {
      "displayName": "Lucille Kaleha",
      "userId": "17017568870215388501"
     },
     "user_tz": -180
    },
    "id": "gQbM7qLCh14O",
    "outputId": "c8ef7455-7df9-4836-c2d9-7f2b9f85b759"
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG4I1Jhph14O"
   },
   "source": [
    "### Plotting a Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3NtrtEqh14O"
   },
   "source": [
    "The confusion matrix is an essential tool in machine learning for evaluating the performance of classification models. It provides a clear representation of how well the model is performing in terms of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgINpTE1h14O"
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 9)  # setting the size o the conf matrix\n",
    "sns.set(font_scale=1.0)  # increasing the font of the values inside the matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, predicted))\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO5eEWZVh14P"
   },
   "source": [
    "To interpret the confusion matrix, we need to understand the following terms:\n",
    "\n",
    "-True Positive (TP): The model correctly predicted the positive class.\n",
    "\n",
    "-False Positive (FP): The model predicted the positive class, but it was actually negative.\n",
    "\n",
    "-True Negative (TN): The model correctly predicted the negative class.\n",
    "\n",
    "-False Negative (FN): The model predicted the negative class, but it was actually positive.\n",
    "\n",
    "Once we have identified these values from the confusion matrix, we can calculate several key metrics to evaluate the performance of our classification model:\n",
    "\n",
    "-Accuracy: The percentage of correct predictions made by the model. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "-Precision: The percentage of positive predictions made by the model that were correct. It is calculated as TP / (TP + FP).\n",
    "\n",
    "-Recall: The percentage of actual positive cases that were correctly predicted by the model. It is calculated as TP / (TP + FN).\n",
    "\n",
    "-F1-score: The harmonic mean of precision and recall. It is a balanced measure that considers both precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqx9iKh7P37l"
   },
   "source": [
    "\n",
    "The logistic regression model achieved an accuracy of 84%, with varying precision, recall, and F1-scores across the different acceptability classes. Class 1 (unacceptable) has the highest precision, recall, and F1-score, indicating good performance. However, for classes 2, 3, and 4, the precision, recall, and F1-scores are lower, indicating difficulty in accurately predicting these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK5B1o2bh14P"
   },
   "source": [
    "### Applying SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBwI0bKCh14P"
   },
   "source": [
    "As we saw earlier we have a large class imbalance in our data, we sometimes will want to create artificial data from the minority class in order to create better class balance. There are multiple methods to do this, but the method we explore here is a pretty standard and simple one, SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxmuHkq0h14Q"
   },
   "outputs": [],
   "source": [
    "# applying smote to see whether there will be any improvement with the prediction\n",
    "\n",
    "# instead of using scikit learn's pipeline, we import from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# just like we did for MinMaxScaler, we instantiate SMOTE within the pipeline\n",
    "pipe_sm = Pipeline(\n",
    "    steps=[\n",
    "        (\"smote\", SMOTE(random_state=23)),\n",
    "        (\"minmaxscaler\", MinMaxScaler()),\n",
    "        (\n",
    "            \"logisticregression\",\n",
    "            LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "pipe_sm.fit(X_train, y_train)\n",
    "\n",
    "predicted_sm = pipe_sm.predict(X_test)\n",
    "pipe_sm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAs0ym150IgK"
   },
   "outputs": [],
   "source": [
    "# incase you encounter the [error No module named 'imblearn'] in the above cell run the command below\n",
    "# !pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nqa2d0Hh14Q"
   },
   "outputs": [],
   "source": [
    "cross_val_score(pipe_sm, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAG14hGdh14R"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpJmbZnth14R"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9, 9)  # setting the size o the conf matrix\n",
    "sns.set(font_scale=1.0)  # increasing the font of the values inside the matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, predicted_sm))\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnbKf9XMQC7M"
   },
   "source": [
    "The model still performed well, but not as well as our baseline model. There could be a few reasons why this happened;\n",
    "\n",
    "* Logistic regression is not particularly sensitive to class imbalance. However, if the dataset was already well-balanced or moderately imbalanced, applying SMOTE might have introduced synthetic examples that deviated from the original data distribution. These artificially generated instances could have led to a loss of important information or introduced noise, which negatively impacted the performance of the logistic regression model.\n",
    "\n",
    "* SMOTE can sometimes lead to overfitting, especially if the synthetic examples generated by SMOTE do not accurately represent the underlying patterns in the original data. Overfitting occurs when the model becomes too specific to the training data and fails to generalize well to new, unseen data. If the synthetic examples from SMOTE dominated the training data, the logistic regression model might have become overly complex and overfitted to the augmented dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nrkdV6ph14S"
   },
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "Decision trees do not assume any specific distribution or relationship between features, making them suitable for a wide range of data scenarios. They can also handle a more complex decision boundary compared to Linear Regression. This flexibility allows decision tree classifier to adapt well to the specific characteristics of the car evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hva9lyyDh14S"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# declare the steps in our pipeline\n",
    "dtc_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"minmaxcaler\", MinMaxScaler()),\n",
    "        (\"dtc\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=23)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "dtc = dtc_pipe.fit(X_train, y_train)\n",
    "\n",
    "dtc_predicted = dtc.predict(X_test)\n",
    "dtc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fynuyk5Ph14T"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, dtc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubl-6NUUh14T"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9, 9)  # setting the size o the conf matrix\n",
    "sns.set(font_scale=1.0)  # increasing the font of the values inside the matrix\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, dtc_predicted))\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI9sy5QxVv1D"
   },
   "source": [
    "The decision tree classifier performed exceptionally well, with an accuracy of 99% on the car evaluation dataset. The precision, recall, and F1-scores for all classes (1, 2, 3, and 4) are consistently high, indicating excellent performance across the board.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpcOLeErh14U"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    " The decision tree classifier remains the superior choice for the car evaluation dataset. It exhibits outstanding performance, achieving near-perfect accuracy and high precision, recall, and F1-scores for all acceptability classes. In contrast, logistic regression achieved a lower accuracy of 84% and demonstrated lower performance, particularly for predicting classes 2, 3, and 4.\n",
    "\n",
    "Therefore, the decision tree classifier remains the recommended model for accurately predicting car acceptability based on the given dataset. This is therefore the model we'll save for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuYHYr5fh14U"
   },
   "outputs": [],
   "source": [
    "# create a context manager to save the model - model.pkl\n",
    "with open(\"model_pkl\", \"wb\") as files:\n",
    "    pickle.dump(dtc, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i0BtRgpXiB_"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "### Deployment\n",
    "\n",
    "In machine learning, deployment refers to the process of taking a trained model and making it available for use in real-world applications. Deploying a model involves integrating it into an operational system or platform, where it can receive input data, make predictions or classifications, and provide valuable insights or actions.\n",
    "\n",
    "Deployment is a crucial step in the machine learning lifecycle as it bridges the gap between developing models and putting them into practical use. It allows organizations to leverage the power of machine learning algorithms and apply them to solve complex problems, automate decision-making processes, improve efficiency, and enhance user experiences.\n",
    "\n",
    "In order to deploy the model we have built for this project, follow along with these resources and let me know how it goes;\n",
    "\n",
    "1.   [Deploying a basic Streamlit app](https://towardsdatascience.com/deploying-a-basic-streamlit-app-ceadae286fd0)\n",
    "2.   [Machine Learning Model Deployment](https://www.analyticsvidhya.com/blog/2021/10/machine-learning-model-deployment-using-streamlit/)\n",
    "3.   [Car Evaluation Deploment Example](https://github.com/kurtispykes/car-evaluation-project/tree/Main)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0Y3ezG5h14a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

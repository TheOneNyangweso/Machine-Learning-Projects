{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits # Scikit-Learn’s set of preformatted digit which is built into the library.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "#ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images data is a three-dimensional array: 1,797 samples, each consisting of an\n",
    "8×8 grid of pixels. Let’s visualize the first hundred of these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={\n",
    "                         'xticks': [], 'yticks': []}, gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with this data within Scikit-Learn, we need a two-dimensional, [n_samples, n_features] representation. We can accomplish this by treating each\\\n",
    "pixel in the image as a feature—that is, by flattening out the pixel arrays so that we have a length-64 array of pixel values representing each digit. Additionally, we need\\\n",
    "the target array, which gives the previously determined label for each digit. These two quantities are built into the digits dataset under the data and target attributes,\\\n",
    "respectively:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a softmax function and not sigmoid since sigmoid only works for binary classification. Softmax on the other hand can be used in multinomial classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array of shape (3, 4) filled with random numbers\n",
    "z = np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, softmax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX = X.T.dot(X)\n",
    "XTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX = X.T.dot(X)\n",
    "XTX_inv = np.linalg.pinv(np.array(XTX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = XTX.dot(XTX_inv)\n",
    "np.around(res, decimals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_full = XTX_inv.dot(X.T).dot(y)\n",
    "w0 = weights_full[0]\n",
    "w = weights_full[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros((X.shape[1], len(np.unique(y))))\n",
    "        self.classes = np.unique(y)\n",
    "        y = np.eye(len(self.classes))[y]\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            scores = np.dot(X, self.weights)\n",
    "            predictions = self.softmax(scores)\n",
    "            gradient = np.dot(X.T, (predictions - y)) / y.shape[0]\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.dot(X, self.weights)\n",
    "        predictions = np.argmax(self.softmax(scores), axis=1)\n",
    "        return self.classes[predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Standardizingthe features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(digits.data)\n",
    "\n",
    "# Getting the labels\n",
    "y = digits.target\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and train the model\n",
    "model = MultinomialLogisticRegression(learning_rate=0.01, num_iterations=300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now comparing with scikit-learn's logistic regression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_sklearn = lr.predict(X_test)\n",
    "\n",
    "print(\"Scikit-learn's implementation accuracy: \",\n",
    "      accuracy_score(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"My implementation accuracy: {round(accuracy_score(y_test, y_pred), 4) * 100}%\")\n",
    "print(\n",
    "    f\"Scikit-learn's implementation accuracy: {round(accuracy_score(y_test, y_pred_sklearn), 4) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y_pred_bin = label_binarize(y_pred, classes=[\n",
    "                            0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y_pred_sklearn_bin = label_binarize(\n",
    "    y_pred_sklearn, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "print(\"Our implementation AUC: \", roc_auc_score(\n",
    "    y_test_bin, y_pred_bin, multi_class='ovr'))\n",
    "print(\"Scikit-learn's implementation AUC: \",\n",
    "      roc_auc_score(y_test_bin, y_pred_sklearn_bin, multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Precision, Recall and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"My implementation classification report: \\n\",\n",
    "      classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Scikit-learn's implementation classification report: \\n\",\n",
    "      classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a histogram of the implementation's predictions\n",
    "sns.histplot(y_pred, color='red', alpha=0.5, label='My Implementation')\n",
    "\n",
    "# Create a histogram of scikit-learn's predictions\n",
    "sns.histplot(y_pred_sklearn, color='blue', alpha=0.5, label='Scikit-Learn')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Create a histogram of the implementation's predictions\n",
    "sns.histplot(y_pred, color='red', alpha=0.5,\n",
    "             label='My Implementation', ax=ax[0], kde=True)\n",
    "\n",
    "# Create a histogram of scikit-learn's predictions\n",
    "sns.histplot(y_pred_sklearn, color='blue', alpha=0.5,\n",
    "             label='Scikit-Learn', ax=ax[0], kde=True)\n",
    "\n",
    "# Add a legend\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot our implementation\n",
    "sns.heatmap(cm_ours, annot=True, fmt='d', ax=ax[1], cmap='Blues')\n",
    "ax[1].set_title('My Implementation')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "ax[1].set_ylabel('True')\n",
    "\n",
    "# Plot scikit-learn's implementation\n",
    "sns.heatmap(cm_sklearn, annot=True, fmt='d', ax=ax[2], cmap='Blues')\n",
    "ax[2].set_title('Scikit-learn\\'s Implementation')\n",
    "ax[2].set_xlabel('Predicted')\n",
    "ax[2].set_ylabel('True')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is also typically having lower values on multiple evaluation metrics.\\\n",
    "This could be due to a variety of reasons:\n",
    "\n",
    "1. **Model Complexity**: The model implemented is a simple multinomial logistic regression model. It's a linear model, which means it may not capture complex relationships between features. On the other hand, models in libraries like scikit-learn often include additional features like regularization, which can help them perform better on complex datasets.\n",
    "\n",
    "2. **Optimization Algorithm**: The model implemented uses gradient descent for optimization, which is a simple and widely used method. However, it might not always be the best choice. Scikit-learn's `LogisticRegression` uses more advanced optimization algorithms (like 'liblinear' or 'lbfgs') that often converge faster and find better solutions.\n",
    "\n",
    "3. **Hyperparameters**: The learning rate and the number of iterations are hyperparameters of the gradient descent algorithm, and they can significantly affect the performance of the model. If they are not set properly, the model might not learn effectively from the data. Scikit-learn's models have mechanisms to tune these hyperparameters.\n",
    "\n",
    "4. **Numerical Stability**: The implementation of certain functions (like softmax) can suffer from numerical instability, leading to incorrect calculations. Scikit-learn has a stable implementations of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main intuition behind SVMs are as follows: rather than simply drawing a zero-width line between the classes, we can draw\\\n",
    "around each line a margin of some width, up to the nearest point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA',\n",
    "                     alpha=0.4)\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVM, the line with the maximum width is chosen as the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Fitting a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  # Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a two-dimensional SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none')\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strength of SVM is its insensitivity to the exact behaviour of distant points.\n",
    "SVM is only foucused on the support vectors as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the model above doesn't change when the no. of training points increrase...This can be confirmed interactively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[a for a in range(20, 200, 20)], ax=fixed(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beyond linear boundaries : Kernel SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the need for kernels, let's see some data that is not linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets._samples_generator import make_circles\n",
    "\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "# Visualizing\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "plot_svc_decision_function(clf, plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will ever be able to separate the data.\\\n",
    "This behaviour is generally summarised as : 'A linear classifier performs poorly for nonlinear boundaries'\\\n",
    "<br>\n",
    "One solution is to project the data into higher dimensions such that a linear separator would be sufficient. This is mostly seen in Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a radial basis function centered on the middle clump\n",
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's visulaize this xtra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "\n",
    "interact(plot_3D, elev=[-90, 90], azip=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing is that all this kernel tricks are implemented in SVM, you just have to specify in the kernel hyperparameter.\\\n",
    "If you want to use radial basis function, you change the kernel from linear to rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the SVM : softening margins\n",
    "This is usually applied when the data has some sort of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, you have to alter the tuning param C, such that it produces the optimal model. You can use cross validation\\\n",
    "or a similar procedure to get the optimal value of  the C hyperparam.\\\n",
    "For now let's see the how changing C affects the final fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='BrBG')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none')\n",
    "\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
